# Glossary of AI Governance Terms

## A

### Abuse Attacks  
Attempts to misuse AI systems for malicious purposes, such as impersonating users, automating phishing attempts, or spreading misinformation. Think of a chatbot being tricked into sending spam.

### Affirmative Consent  
Users must clearly and voluntarily agree to how their data will be used. In AI governance, this protects people’s rights and avoids sneaky data grabs.

### AI Actors  
Entities involved in the AI system’s lifecycle—like developers, data scientists, deployers, auditors, and users. Each actor has different responsibilities, and governance ensures they stay accountable.

### AI Application Security  
Practices to secure AI-powered applications from threats like data leaks, prompt injection, or adversarial attacks. Like putting locks on a digital robot’s doors.

### AI Governance  
The overall system of rules, practices, tools, and roles that ensure AI is developed and used in ways that are ethical, legal, and trustworthy.

### AI Impact Assessment (AIIA)  
An evaluation to understand and document how an AI system might affect people, society, or the environment. Helps prevent surprises—especially harmful ones.

### AI Lifecycle  
The entire journey of an AI system—from development and training to deployment, monitoring, updates, and decommissioning. Governance must span all stages.

### AI Model  
A trained system that uses algorithms and data to make decisions or predictions. Like a recipe that turns ingredients (data) into a finished dish (output).

### AI Model Discovery  
Finding all AI models used in an organization—including ones developed informally ("shadow AI")—so they can be documented, monitored, and governed.

### AI Models in Public Clouds, SaaS Applications, and Private Environments  
Where AI models live—public clouds (like AWS), third-party platforms (like Salesforce), or internal servers. Each has unique risks and governance needs.

### AI Pipelines  
The automated series of steps (data cleaning, training, testing, deployment) involved in creating and maintaining AI systems. Like a factory line for algorithms.

### AI Readiness Assessments  
Evaluations that check if an organization has the right policies, tools, and culture to responsibly implement AI. It’s like a check-up before liftoff.

### AI Risk Assessment  
A structured way to identify what could go wrong with an AI system—like unfair outcomes or data leaks—and how bad the impact might be.

### AI Risk Management  
Ongoing efforts to reduce AI-related risks using controls, testing, oversight, and feedback. Like airbags and seatbelts for AI systems.

### AI Trustworthiness  
Whether an AI system is seen as reliable, fair, and safe. Built through transparency, consistent results, and respectful handling of data.

### AI Usage Notice  
A statement that informs users they’re interacting with AI. Helps avoid confusion and builds trust. Think: “This chat is powered by AI.”

### Algorithms  
Step-by-step instructions that help machines solve problems. In AI, algorithms help systems "learn" from data and make predictions.

### Anonymization  
Scrubbing data of personal identifiers so individuals can’t be identified. Used to protect privacy, especially in training datasets.

### Assess Risks and Classify AI Models  
A governance step where AI systems are categorized by how risky they are—like “low-risk chatbot” vs. “high-risk facial recognition.”

### Autoregressive Models  
Models that predict one element at a time based on previous ones. Common in text generation—like writing a sentence word by word using context.

---

## B

### Bias  
When AI systems produce unfair results because of skewed data or design flaws. For example, a loan model favoring one group over another.

### Bill No. 2338/2023, the Brazilian AI Bill  
Brazil’s draft legislation to regulate AI use, emphasizing fairness, human rights, and transparency.

### Biometric Information Privacy Act (BIPA)  
An Illinois law that protects biometric data—like face or fingerprint scans. It’s important for companies using AI-powered recognition.

### Copyright  
Legal protection for creative works. Using copyrighted content to train AI can lead to violations if done without permission.

### Cross-domain Collaboration  
When departments like legal, compliance, and engineering work together to ensure AI systems are responsibly governed.

---

## C

### Canada Artificial Intelligence and Data Act (AIDA)  
Proposed Canadian law that sets rules for high-impact AI systems, aiming to keep them safe, fair, and accountable.

### CCPA/CPRA  
California privacy laws that give people rights over their data, including how AI uses it.

### Convolutional Neural Networks (CNNs)  
AI models that excel at processing images, like detecting faces or classifying objects.

### Content Filtering  
AI systems that screen out harmful, offensive, or inappropriate content—used on social platforms and in comment moderation.

### Conversion  
Transforming data into formats that AI systems can use. For instance, turning handwritten notes into digital text.

---

## D

### Data + AI Mapping  
Charting where data goes and how AI systems use it. This clarity helps prevent risks and keeps organizations compliant.

### Data Exfiltration Attacks  
When sensitive data is stolen—often through AI vulnerabilities. Think of a chatbot leaking confidential files.

### Data Leakage Prevention  
Techniques that block sensitive data from being exposed during AI use or training.

### Data Minimization  
Only collecting what you need. It reduces risk and respects privacy. Don’t vacuum up oceans of data when a glass will do.

### Data Poisoning Prevention  
Protecting models from being trained on harmful or manipulated data, which could distort outputs or create vulnerabilities.

### Data Privacy  
Ensuring people’s personal information is used respectfully and safely, especially when powering AI.

### Data Protection Principles  
Core rules for handling data—like transparency, fairness, and limiting how data is used.

### Data Sources  
Where AI gets its information—like websites, customer forms, or IoT sensors.

### Data Subject Rights (DSR) Fulfillments  
Processes that let individuals view, edit, or delete their data—especially under laws like GDPR or CCPA.

### Denial-of-Service (DoS) Protection  
Defenses that help AI systems stay online and responsive during traffic spikes or attacks.

### Denial of Service  
An attack that floods a system with requests to make it crash. AI APIs need safeguards against this.

### Diffusion Models  
Image generation models that start with noise and refine it into something meaningful—like creating art from static.

### Discrimination  
Unfair treatment of individuals or groups by an AI system, often due to biased data or flawed logic.

### Discriminative AI  
Models that label or classify input data. Unlike generative AI, they don’t create new content—just categorize it.

### Discover and Catalog AI Models  
The process of identifying every AI model in use across an organization, recording its details, and evaluating its risks.

---

## E

### Energy Consumption  
The electricity used to train and run AI models. Some large models consume as much power as small towns—making efficiency an important goal.

### Ethical AI  
AI that aligns with core human values like fairness, dignity, and transparency.

### EU’s AI Act  
Europe’s upcoming law that classifies AI systems by risk levels and sets rules for each level. High-risk systems (like job screeners) face the most scrutiny.

### Evasion Attacks  
Tricking an AI system into making wrong decisions—for example, altering a stop sign to confuse a self-driving car.

### Explainability  
Making AI decisions understandable to humans. This helps build trust, debug issues, and comply with legal standards.

### Explainability and Privacy Notice  
A plain-language document that tells users what the AI does and how it handles their data.

---

## F

### Foundation Models  
Large models trained on huge amounts of data that can be fine-tuned for many tasks. Examples: GPT, BERT.

---

## G

### GANs (Generative Adversarial Networks)  
Two models working in tandem—one generates fake content, and the other tries to detect the fake. Together, they improve realism.

### General Data Protection Regulation (GDPR)  
The EU’s landmark privacy law that affects how AI systems handle personal data.

### General-Purpose AI  
Flexible AI systems that can do many things—not just one task. Their broad use makes governance more complex.

### Generative AI  
AI that creates new content, like writing poems or generating images. It raises unique challenges around originality and misinformation.

### GPT  
A family of autoregressive language models (like ChatGPT) that generate natural-sounding text from prompts.

---

## H

### Hallucinatory Responses  
When an AI makes up answers that sound right but are wrong or misleading. Think of it as confident nonsense.

---

## I

### Image Classification  
When AI labels what’s in a photo, like “dog,” “tree,” or “pizza.”

### Impact  
The severity of harm an AI system could cause. Used in risk assessments to prioritize attention.

### Inference Runtime  
How long it takes for an AI system to respond to input. Affects user experience and system performance.

### Inputs and Outputs  
What goes into an AI system (like a user question) and what comes out (like the answer). Governance ensures both are safe and meaningful.

### Inline Classification  
Tagging or labeling data in real time as it flows through a system—like flagging customer chats for urgency.

### Integration Points  
Places where the AI system connects with the rest of your technology—like APIs or dashboards.

### Intellectual Property  
Ownership rights over inventions or creative works—including AI-generated content or proprietary models.

### Insecure Output Handling  
When an AI system outputs unsafe or manipulated content because it didn’t properly validate its response.

---

## J

### Jailbreaking  
Manipulating an AI system (especially LLMs) to bypass its safety filters. For example, tricking a chatbot into giving harmful instructions by rewording prompts creatively. Governance involves guardrails to detect and prevent this.

---

## L

### Large Language Models (LLMs)  
AI systems trained on vast text datasets to understand and generate human-like language. They power tools like ChatGPT, Bard, and Claude.

### Likelihood  
In risk assessments, this means the chance something harmful might happen. Used with "impact" to calculate overall risk.

### Long Short-Term Memory (LSTM)  
A special type of neural network designed to understand sequences over time—like predicting the next word in a sentence or forecasting trends.

---

## M

### Malicious Use  
Using AI in harmful ways, like spreading misinformation, automating scams, or generating deepfakes. Strong governance anticipates and blocks these risks.

### Map and Monitor Data + AI Flows  
Tracking how data moves through AI systems so nothing gets lost, stolen, or misused. Think of it like plumbing diagrams for your data.

### Model Architecture  
The internal design or “blueprint” of an AI model. Different architectures (like CNNs, transformers, or LSTMs) influence what the model is good at.

### Model Cards  
A short document explaining what a model does, its limitations, and how it should (and shouldn’t) be used. Like a warning label and instruction manual in one.

### Model Monitoring  
Watching AI systems once they’re live to catch issues like performance drift, failures, or harmful outputs.

### Model Operations  
The day-to-day management of AI models—deployment, updates, scaling, and troubleshooting. Often called **MLOps**.

### Model Privacy  
Protecting private information that an AI model might “remember” from its training data. Critical for avoiding data leaks.

### Model Purpose  
Why the model was built—like to summarize documents, detect fraud, or answer questions. Governance ensures the model isn’t misused.

### Model Risks  
The possible problems that might come from using a particular model, including bias, misuse, or errors.

### Model Theft  
When someone copies or reverse-engineers your AI system to steal intellectual property. Safeguards include watermarking and access controls.

---

## N

### Named-Entity Recognition  
AI that picks out specific things in text—like people, places, or dates. Useful in legal tech, healthcare, or customer support.

### National Institute of Standards and Technology (NIST)  
A U.S. agency that provides guidelines to help organizations build secure and trustworthy AI.

### NIST AI Risk Management Framework (RMF)  
A step-by-step approach from NIST for identifying, measuring, and managing AI risks in a structured way.

---

## O

### OECD’s High-Level AI Risk Management Interoperability Framework  
International guidelines that promote cooperation across borders so AI governance efforts work together instead of clashing.

### Optical Character Recognition (OCR)  
AI that converts images of text (like scanned documents) into editable and searchable text.

### Open Worldwide Application Security Project (OWASP)  
A global community focused on improving software security. OWASP now also tracks risks related to AI and LLMs.

### Overreliance  
Trusting AI systems too much or using them beyond their limits. Governance means educating users and keeping humans in the loop.

---

## P

### Permission and Entitlement Controls  
Settings that determine who can use, edit, or manage an AI model or its data—essential for limiting risk.

### Phishing Prevention  
Using AI to spot and stop phishing attacks—or protecting AI tools from being used to create smarter scams.

### Prompt Firewall  
A layer that screens incoming prompts to remove harmful or manipulative content before it reaches an LLM.

### Prompt Injection Attacks  
Tricking an LLM into doing something bad by hiding instructions in a prompt. Like telling it to ignore safety rules using sneaky language.

### Publicly-Accessible Data  
Data that’s freely available online. Just because it's public doesn't always mean it's okay to use for AI training—especially under privacy laws.

### Purpose Limitation  
Only using data for the reason you collected it. Stops data from being reused in harmful or misleading ways.

---

## R

### Redaction  
Removing sensitive details—like names or Social Security numbers—from data before it’s used by AI.

### Retrieval Augmented Generation (RAG) Processes  
A method where a language model looks up real documents to help answer questions, reducing hallucinations and improving accuracy.

### Retrieval Firewall  
A filter that decides what information can be used in RAG workflows to prevent toxic or unauthorized content from leaking in.

### Risk Treatment  
Deciding what to do with an identified risk—like reducing it, accepting it, or avoiding it by changing your system.

---

## S

### Sanitization  
Cleaning inputs and outputs to remove toxic, sensitive, or malicious content. Like washing your data before letting it near a model.

### Sentiment Analysis  
AI that figures out whether text sounds positive, negative, or neutral. Used in marketing, customer support, and beyond.

### Shadow AI  
When teams or individuals use AI tools without formal approval, risking security, compliance, and oversight.

### Singapore AI Governance Framework  
A set of AI governance principles from Singapore that aim to promote ethical innovation and responsible deployment.

### Standardization  
Creating consistent rules, formats, or processes so AI tools work together safely and predictably.

### Storage Limitation  
Keeping data only as long as needed—important for privacy, compliance, and reducing unnecessary risk.

### Supply Chain Vulnerabilities  
Risks from third-party tools or models you integrate into your system—like pre-trained models with unknown training data or bugs.

---

## T

### Target Classes  
The categories an AI model predicts—like "spam" vs. "not spam" or "urgent" vs. "non-urgent."

### Text Generation Models  
AI that writes new content from scratch, based on a prompt. Used in chatbots, writing tools, and creative AI.

### Text-to-Image Models  
Models that turn text prompts into images—like asking for “a pink elephant playing drums” and getting a visual result.

### Text-to-Video Generation  
AI that turns descriptions into short video clips. Still developing, but promising for ads, art, and entertainment.

### Third-Party AI Vendor Risk Assessment  
Evaluating outside AI providers to make sure their tools meet your standards for privacy, security, and ethics.

### Toxicity  
Offensive or harmful language generated by AI. Tools like filters and moderation layers are used to detect and block it.

---

## U

### User Data  
Information collected from users—such as messages, preferences, or images. AI governance protects how it’s collected, stored, and used.

### User Guides  
Friendly documents that help people understand what an AI system does, how to use it safely, and what to expect.
